XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

Lesson1:

- How to speed up your computation:
  - Faster clock : Shorter time for computation but way more power.
  - More work per cycle: There is a limit to the instruction level parallelism
  - Parallelism: Many smaller simpler processors doing the tasks.

- Modern GPUs
 - Thousands of ALUs
 - Hundereds of processors
 - Tens of thousands of concurrent threads

- GPGPU = General processing on a graphics processing units

- Transistors and becoming smaller but the clock frequency is becoming flat. So
  are getting faster performance because more transistors are there not because
  of clock speed.
- But more transistors means more power and more heat. That's a problem.

- CPU : Have a complex control hardware which means higher performance and flexibility
        but expensive in terms of power.
- GPU : Simpler control hardware means power efficient, more HW for computation
        but less flexible and more restrictive.

- Note that Latency and Throughput are the key factors. CPU optimizes for Latency.
  It wants to complete the tasks quicker. But GPU wants to process as many tasks as
  possible at a time and hence optimizes for throughput.

- GPU design tenets:
  - Lots of simple compute units. Trade simple control for more compute
  - Explicitly cater to parallel programming.
  - Optimize for throughput not latency.

- CUDA:
  - Allows us to program both the CPU and GPU in one program.
  - CPU is the host. It will be in plain C.
  - GPU is the device. It will also be in C but with some extensions.
  - The compiler will split the code and run the respective parts on CPU/GPU.
  - Assumes that the CPU/GPU have separate memories.
  - CPU runs the main program and performs the following:
    - Control data flows from CPU to GPU (cuda memcpy)
    - Control data flows from GPU to CPU (cuda memcpy)
    - Allocate memory on the GPU (cuda malloc)
    - Launch kernel on the GPU
  - NOTE that the CPU is the boss.

- Typical GPU program
  1. CPU allocates storage on GPU.
  2. CPU copies data from the CPU to the GPU.
  3. CPU launches kernels (equal to number of threads) on GPU to process the data.
  4. CPU copies results from the CPU back to the GPU.

- GPU programming is favored for applications with a good ratio of processing to
  communication.

- BIG IDEA OF CUDA:
  - Kernels look like serial programs. You write programs as if it will run on
    one thread but the GPU will run that parallel block on many threads. (SPMD)
  - GPUs are good at:
    - Efficiently launching lots of threads.
    - Running lots of threads in parallel.

- Running a kernel
  - The first is the number of blocks and the second is the number of threads per block
  - Can run many blocks at once.
  - Max number of threads per block is 512 for old and 1024 for new
  kernel<<<BLOCK_SIZE, THREADS_PER_BLOCK>>>(VARIABLES);
  - Each thread knows its index in the block and also knows the index of the block
  - Both of the parameters above can be 1,2 or 3 dimensional. For eg. If you
    have 128x128 image you could have 1x128 blocks of 128x1 threads or you could
    have a 8x8 grid of blocks where each block is of size 8x8
  kernel<<<GRID_OF_BLOCKS, BLOCKS_OF_THREADS>>>(VARIABLES);
  - You can use the dim3(x,y,z) to define the dimensionality of each of those
    parameters. It defaults y and z to 1 when not specified.
  kernel<<<dim3(bx,by,bz), dim3(tx,ty,tz), shmem>>>(VARIABLES)
  - NOTE: Why are you able to pass some host variables directly (like numRows and
    numCols in all the image kernels)
      https://stackoverflow.com/questions/6499036/kernel-parameter-passing-in-cuda
  - Here we have
    - Dimensionality of thread of blocks
    - Dimensionality of block of threads
    - Shared memory per block in bytes (defaulted to 0)
  - Each thread knows
    - threadIdx: x,y,z dimensions of the thread in the block.
    - blockDim: Size of the block of threads (How many threads in block)
    - blockIdx: x,y,z dimensions of block in the grid
    - gridDim: Size of the grid of blocks
  - A __device__ or __global__ function runs on the GPU /Device functions can only
    be called from other device or global functions. __device__ functions cannot
    be called from host code.
  - Note that in each SM the threads are aggregated into sets of 32 called warps.
    The warps run the same code and run in an SIMD fashion. This fact can sometimes
    be used to optimize your functions.
      - https://stackoverflow.com/questions/10460742/how-do-cuda-blocks-warps-threads-map-onto-cuda-cores
      - https://stackoverflow.com/questions/10729185/removing-syncthreads-in-cuda-warp-level-reduction

- Map
  - Here we have elements we want to process in parallel
  - There is a function we use to process each element
  - So we have an abstraction Map(ELEMENTS, FUNCTION)
  - GPUs are good at maps:
    - GPUS have many parallel processors
    - Its optimized for throughput and not latency

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

Lesson 2:

- Threads are working together on the same shared memory:
  - They may be accessing mutually exclusive data.
  - Dumping all of their results to the same spot.
  - Sharing partial results between them.

- Different patterns of communication:
  - Map:
    - What we've done till now. Tasks read and write to specific data elements
    - Like pixels in a image or indexes in an array.
    - One to one correspondence input and out.
    - Not flexible. Like can't average.
  - Gather:
    - Many input to one output.
    - Averaging over the window in an array.
    - Mean filtering over an image.
  - Scatter:
    - One input is spread across different output.
    - Like spreading the pixel value among surrounding pixels.
    - Scattering the results over the memory.
    - Problem several threads are trying to access the same memory at same time.
    - Each thread has to compute where to write the result.
  NOTE: The main difference between gather and scatter is that in gather non adjacent
  memory locations are being mapped to adjacent memory locations. In scatter on the
  other hand adjacent input locations are being mapped to memory locations that are
  computed using some criteria.
  - Stencil:
    - Tasks read input from a fixed neighborhood in an array
    - There is often a lot of data reuse when the stencil masks overlap
    - Each cell is of course gonna be read as many times as there are elements in
      a stencil.
    - A stencil must generate an output for every input.
  - Transpose:
    - Transposing an array expressed in row major order to a column major order.
    - It can be expressed as a gather or scatter (see NOTE above for how its different.)
    - Can be operated on arrays, matrices, images, data structures (Combining certain
      fields together to get a Structure of Arrays from an Array of Structures)
    - Tasks reorder data elements in memory.
    - It is one to one.
  NOTE:
    - Map and transpose are one to one.
    - A stencil needs to have as many outputs as input. Its a special form of gather.
    - Gather (Many to one). Mapped to adjacent memory location.
    - Scatter (one to many). Need to compute where data is being mapped to.
    - Reduce (all to one) like sum or average
    - Scan / Sort (all to all) All of the input can affect the destination of the output

- GPU Hardware:
  - Divide into smaller computations called kernel.
  - Thread blocks are a group of threads that cooperate to solve a subproblem.

- Threads, blocks and SMs:
  - A GPU is a collection of streaming multiprocessors or SM.
  - A SM has a bunch of simple processors and a memory.
  - When you have threads arranged in blocks the GPU will take care of assigning
    the blocks to the SM.
  NOTE: An SM can run more than one thread block but a thread block cannot run in
  more than one SM. Also note that only threads in the same block may cooperate.
  - So CUDA makes few guarantees about when and where the thread blocks can be run.
  - This flexibility allows hardware to run efficiently, no waiting and scalability.
  - The cons are there are no communication between blocks and no assumption on where
    blocks are running.
  NOTE: A dead lock is when one thread is waiting for the result of another but the
  thread has already been completed.
  - Threads and blocks must complete. They cannot be hanging around forever.
  NOTE: cudaDeviceSynchronize() ensure all the blocks execute.

- CUDA guarantees that
  - All threads in a block run on the same SM at the same time.
  - All the blocks of 1 kernel finish before the blocks of the next kernel are run.

- Memory model:
  - All the threads have local memory.
  - The threads in thread blocks have access to same shared memory which sits on the SM directly.
  - There is also global memory which is accessible to threads everywhere.
  - This is the GPU. The CPU of course has its own host memory.
  - Usually data is copied from the host to the global memory before launching kernels
    to work on them.

- Synchronization:
  - Having this shared and global memory means that threads can work together.
  - But what if one attempt to read a result before another writes it. Hence the
    need to synchronize.
  - Barrier:
    - Point in the program where threads wait till all the threads have reached the
      point.
    - NOTE: Each read write from shared memory needs to be followed up by a barrier.
      Eg:
        // MOving values one idx left in a shared array
        int idx = threadIdx.x;
        __shared__ int array[128]; // Again note this is shared among threads in the same block

        array[idx] = idx; // Writing : Need to sync threads
        __syncThreads();

        if(idx<127)
        {
          // Important to separate reads and writes from shared memories to prevent collisions
          int temp  = array[idx+1]; // Reading : Need to sync threads
          __syncThreads();

          array[idx] = temp; // Writing : Need to sync threads
          __syncThreads();
        }
      - There is an implicit barrier between kernels. All blocks in a kernel must
        end before the next thread executes.

- High level stratergies:

  1. Try to be smart with memory
  - Maximize the math per memory (arithmetic intensity) by either upping the math per thread or
    reducing the time spent on memory access.
  - Local>Shared>>Global>>CPU(host) in terms of speed.
  - So you wanna move frequently accessed data to fast memory.
  - Another way is to use a coalesce the access to global memory.
    - When a thread wants to use some global memory it will usually grab a chunk
      of data around it. Thus if surrounding threads are accessing contiguous data
      this chunk of global memory can be reused. Data is coalesced.
    - If the threads are accessing scattered data (strided or random) then more memory
      transactions ie more chunks need to be accessed.

    2. Avoid thread divergence
    - Threads that do different things due to loops dependent on the id or some
      conditional logic or any reason for them to not be performing the exact same
      operation
    - The GPU likes to perform the same operations on all threads. So some efficiency
      is lost when some threads have more operations than others.


- Atomic:
  - Helps avoid race conditions (collisions) where different threads are trying to read write
    modify the same memory.
  - Some examples are atomicAdd(), atomicMin(), atomicCAS() etc.
  - Limitations:
    - Only certain operations and data types are supported
    - You can work around this using the atomicCAS(). Check it out when possible.
    - No ordering constraint. Problem is that floating point arithmetic is not associative.
    - Serializes the operations which can make atomic very slow.

- NOTE: So when you're evaluating time look for both memory constraints and
  synchronization schemes used.

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

Lesson 3:

Complexity:
  - Ideal scaling: Work reduces linearly with linear increase in processors.
  - Need to talk about step complexity and work complexity.
  - Step complexity is the number of steps in an algorithm.
  - Work complexity is the sum of individual units done by all.
  - A parallel algorithm is work efficient if its work complexity is asymptotically the same.

Reduce:
  - All to one like a sum
  - It has 2 inputs:
    - Set of elements
    - Reduction operator that operates on the arrays
  - Operators need to have the characteristics:
    - Binary : Operations should be on 2 vals only.
    - Associative : (a+b)+c = a+(b+c) where + is any operator
    - Example : Multiply, add, logical or, logical and, min, max
  - Serial operations for reductions take n-1 operations on an array size.
    The work and step complexity is same n-1 or O(n)
  - Parallel reduce:
    - Sequential  reduce ((a+b)+c)+d = (a+b) + (c+d) Parallel reduce
    - This is why associative property was a must.
    - Now both have same work of 3 operations (+) but if you formulate the steps
      because (a+b) and (c+d) can be done in parallel the steps reduces to 2 from
      3.
    - The step complexity has gone down from n to logn. But in real life you
      don't always have n/2 processors to do n/2 operations in the first step.
  - NOTE: See reduce.cu on how to do the reduce method. Also note the difference
    in using shared memory vs global memory. Notice how you're incurring that
    additional copy penalty of copying the global data to shared memory of a thread
    block in order to have faster access and not have to do global memory lookups.
    Syntax for shared memory kernel:
    shmem_reduce_kernel<<<blocks, threads, threads * sizeof(float)>>>
        (d_out, d_in);
    Here the 3rd entry in the bracket is the shared memory size you wanna allocate
    which is of course equal to the number of threads times the size of the datatype
    you're working with.
  - The shared memory variant in reduce.cu is doing 3 times less read write combos
    to the global memory. In the global memory case in each loop you are reading from
    and writing to global memory and you get a GP of reads and writes as you progress
    in steps. But in the shared memory case you are only reading all the global values once
    (1024 reads) and you are writing to global memory only once (1 write) per kernel.
  - Suggested optimizations beyond just converting to shared memory:
    - Process multiple items per thread instead of just one. (meaning?? Maybe run fewer blocks and compute more than one entry in intermediate)
    - Perform the first step of the reduction when copying data from global to shared (trivial)
    - Utilize the fact that warp threads finish synchronously.
      - https://stackoverflow.com/questions/10729185/removing-syncthreads-in-cuda-warp-level-reduction

  Scan:
    - It basically applies an operation like a scan. So the ouput at an index depends
      on all the elements BEFORE (MAY OR MAY NOT including it itself).
    - Example for ADD operation:
      - Input : 1 2 3 4
      - Output: 1 3 6 10
    - This might seem difficult to parallelize.
    - Lots of uses of scan such as compaction and allocation (What are they?)
    - Scan has also been used for sparse matrix optimization, quicksort and data compression.
    - Inputs:
      - Array (Like reduce)
      - Binary associative operator (Like reduce)
      - Also needs an identity element.
    - There are 2 types of scan:
      - Exclusive: Is dependent on all the elements before it NOT including it.
                  (Hence first element is identity)
      - Inclusive: Is dependent on all the elements before it AND including it.
    1. So one stratergy for an inclusive scan might be:
      - For the k-th element run reduce for 0->k. Hence for a n element array
        run reduce n times.
      - Thus work is going to be O(n^2) (n*(n+1)/2 operations)
      - But steps will still be bounded by the max reduce operation which will
        be for element n. So O(logn).
      - Super inefficient in terms of work.
    2. Hillis/Steele inclusive scan
      - Procedure:
        - From 2 to n add 1 element to your left. Keep the elements with no left
          neighbour in this case just the first element as is.
        - Next for 3 to n add the element 2 to the left and keep 1 and 2 as is
        - Next for 5 to n add the element 4 to the left and keep 1->4 as is
        - Repeat this doubling to 8 16 ... n/2 and basically add i to 2^i left neighbor
      - Eg: For n=8 it'll be just 3 steps and you'll get the scan.
      - The total work is max n elements times the number of steps which is logn
        So work is nlogn.
      - The step size is logn since the neighbour access step is jumping by a factor
        of 2.
      - Video: https://www.youtube.com/watch?v=RdfmxfZBHpo
    3. Blelloch exclusive scan
      - 2 steps: Reduce and Downsweep
      - First step is to do the reduce as normal. While keeping track of the positions
      - Then for downsweep look at this video. Its kinda complex to explain verbally.
        Its just like a reverse reduction using the values gathered in the reduce steps.
      - Video: https://www.youtube.com/watch?v=mmYv3Haj6uc
      - The reduce phase has O(logn) step (Has 2 logn steps) complexity and O(n)
        work complexity.
      - So this scan method has double the number of steps than Hillis/Steele but
        considerably less work than it.

    - How to choose between 2. and 3. ?
      - Hillis Steele is better in the number of steps but Blelloch has better work complexity.
      - If more work than processors. Then we prefer 3. Else prefer 2.
      - A lot of algorithms have different work/processor ratio. So switch between
        these depending on this ratio at that point.

    - Summary:
      - Serial : n steps n work
      - Hillis steele: logn steps nlogn work
      - Blelloch: 2logn steps n work

  Histogram:
    - Cumulative distribution: Sum of bin values before me. Use exclusive scan.
    - Naively updating bins in threads will lead to race conditions which are collisions.
    - A simple increment involves reading the bin and storing it in a register, incrementing
      in a local register and writing the bin value back to global memory. So if multiple
      threads clash at these ops you're bound to get wrong bin values.
    - 3 ways to solve it here:
      - 1. Atomics:
        What if we could combine the 3 ops and make it 1 op. This would prevent
        clashing. This is possible through atomic. It basically locks down the
        global memory location while another thread is doing the 3 ops. Diadvantage
        it serializes the bin access since only one thread can access a bin memory location
        at a time. The higher the bins the less chances of collisions. Look at histo.cu for this.
      - 2.  Per thread histograms, then reduce:
        If we have n elements b bins and t threads. Make a local histogram per
        thread where each histogram is made by SERIALLY allocating n/t elements into
        b bins. Then just combine the bins individually using reduce on the histograms.
        It would be faster than running the reduce on the bins.
      - 3. Sort, then reduce by key:
        - Think of entries in key value pairs where the key is the bin and value
          is the value. So you sort by key(bins) first and then reduce all the entries
          of the same bin to get the final value of that bin.
        - Will be discussed in future lessons.

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

Lesson 4:
- Scan is useful way to parallelize a certain class of problems recurrence relationships

- General Applications of Scan:
  - Compact:
      - If we only care about doing computation on a certain subset of the data.
      - Basic structure:
        - Input : A set
        - Predicate : Takes in the set and retruns a boolean. Sort of a mask being
          created from a filter.
        - Output: The set filtered by the predicate to get a sparse output (The
          position in the set remains same and the false elements are just null)
          or dense (No blank spaces the filtered data is contiguous.)
        - NOTE: A sparse construct would require running the same number of threads as the
          original set. But most of those threads would not be utilized. But in
          a dense representation a lower number of fully utilized threads would
          be executed. So dense representation is more efficient. We don't want
          idle threads.
        - Use compact when we are throwing away a large number of elements and
          the surviving elements require heavy computation.
        - Core algorithm:
          1. Predicate: On each element
          2. Scan In array: True is 1 and False is 0
          3. Exlusive sum scan: Output is scatter addresses for compacted arrays.
          4. Scatter input into output using addresses. If predicate is true then take
             the element and copy it to the address defined by the scan output.
        - Note that steps 1,2,3 are independent of size but 4 will be faster if more
          things are filtered out.

    - Allocate:
      - Compact is a 1 and 0 purely binary way to include and exclude things.
      - But maybe we want to generalize that. Maybe the output on applying the
        predicate produces more than a binary output. For example in clipping triangles
        on a screen if the triangle is cut by the screen the resulting polygon will
        need to be cut into triangles. So in allocate we want to be able to gather
        both the complete triangles and the newly formed triangles in a single array.
        That is allocate.
      - One possible strategy:
        - Allocate max possible space in intermediate array.
          - Eg: A triangle can be at max be clipped so that we have to make 5 triangles
            from the resulting 7 sided polygon. So you could allocate 5*numTriangles.
        - Compact the result.
        - Wasteful in space.
        - Requires scanning large intermediate array.
      - Better strategy:
        - Have the predicate output the number of allocations required.
        - Exclusive scan that predicate output.
        - Use the output of the scan and the non zero predicate output to get the
          address of the contiguous block. Use the output of the predicate to decide
          the size of that contiguous block.

      - Segmented scan:
        - Split the array into segments with a separate array marking the start of the
          segments. Do a scan on each of those individual segments.
        - You could have an inclusive or exclusive segmented scan.
        - See online on how to best solve this.

      - Sparse Matrix Dense vector Multiplication (SpMv)
        - Dense matrices: You wanna store all the elements.
        - Sparse matrices: You wanna ignore the zeros.
        - So you want a compact representation of sparse matrices and also a way to
          multiply them with a dense vector.
        - CSR (Compressed Sparse Row):
          - Value: Contains all the non zero data
          - Column: Contains the columns in which the data lie
          - Rowptr: Contains the index/address of the first elements in the row
                    from the Value array above. (Ie index of the first non zero
                    element in a row in the Value vector above.)
        - Matrix multiplication with dense vector:
          1. Create a segmented representation using the value vector and the row pointer.
             The row pointer contains indexes in the value vector where rows begin.
          2. GATHER the vector values using the column vector.
          3. Then use a MAP to pairwise multiply the vectors from 1. and 2.
          4. Then finally add using a SEGMENTED SCAN or more optimally a SEGMENTED
             REDUCE.
- Sorting:

    Find Efficient Parallel Algorithms:
    - Keep HW busy (Lots of threads)
    - Limit branch divergence
    - Prefer coalesced memory access

    Odd-Even sort (Brick sort)
    - Mark indexes as odd even. Then have the even indexes look to the left and
      swap if out of order. Then have them look on the right and swap if they
      are out of order. Repeat till sort achieved.
    - Step complexity of O(n) but work on O(n^2)(Number of comparisons)
    - You can do the comparison in a step in parallel. So it speeds things up.

    Merge sort
    - It is a great sorting algorithm for the GPU since it is a divide and conquer algo.
    - You basically split up the data into unit sized chunks and intelligently merge
      sorted arrays together to get the combined totally sorted array.
    - So you start with n elements, perform n/2 merges, to get n/2 sorted arrays,
      then n/4 merges to get n/4 sorted arrays and so on.
    - Since there are n elements and logn operations the work complexity is O(nlogn)
      and the step complexity is O(n).
    - Since the array size increases from a single element to large values, it
      makes sense to run this in multiple stages depending on the array size at the
      step.
        - Do 1 merge per thread. (For small arrays)
        - Do 1 merge per thread block. (For medium arrays)
        - DO 1 big merge task spread across multiple SMs. (Final merge)
    - Parallel merge:
      - The index of the number in its own list is the thread id.
      - Then find the index of the same number in the other array by doing a binary
        search over the other array.
      - The final index is hence just the sum of these 2 array indices.
    - For the 1 big merge task if we force the task to only 1 SM most of the SMs
      are idle. So we need to spread that task across multiple SMs.
      - So you make splitters which are basically the points at some regular intervals
        say n in each list.
      - Then you parallel merge the splitters as above.
      - Then you find the list of elements between the splitters in each list. This
        of course cannot be more than 2*n. It can at max be 2*n. Then parallel merge them.

      Sorting networks:
      - Oblivious: Behavior is independent of some aspect of the program. Good
        for GPUs that prefer simple processes.
          - So in case of sorting an oblivious algorithm will have the same time
            for a sorted or unsorted algorithm.
      - Sorting sequences are a collection of binary comparisons that result in
        sorted sequences
      - Bitonic sorting:
        - Bitonic sequence is one that changes direction only once. Increase first
          then decrease or vice versa.
        - We can stack the monotonic sequences over each other and take the max
          and min and hence split it into 2 bitonic sequences.
      - Read more about sorting networks

      Radix Sort:
      - One of the best performing sort algorithms on the GPU:
      - Steps:
        - Start with the least significant bit. Move the numbers with 0s on top
          and 1s below while maintaining the order.
        - Move to the next most significant bit and then move the 0s up and push
          1s down while preserving order.
        - Repeat till all bits scrolled over.
      - The amount of work is propotional to bits and the number of numbers. But
        since the number of bits is usually constant (32 bit). So the work complexity
        is O(n).
      - The first step reminds you of the compact step with the predicate (i&1==0)
        and then run a scan to get the addresses of the 0 half. Then we can begin
        with the last index and do a scan on the 1 (i&1==1) half and get the indexes for them.
      - The same concept can be expanded and we could do a 16 way split by taking
        the first 4 bits instead of only the least significant bit. This will make
        the sort faster.

      Quick Sort:
      - One of the most efficient algorithms on the CPU but a bit complex to implement
        on the GPU.
      - Steps:
          - Pick a pivot (center usually). Swap it with the end element.
          - Pick the right most element which is smaller than the pivot. Pick the
            left most element greater than the pivot.
          - If the left index is less than the right index swap them.
          - If the left is greater than the right, then swap the right element
            with the pivot at the end.
          - The pivot is in its correct position in the final sorted array.
          - Perform a quick sort on the list before the pivot and the list after
            the pivot.
          - Keep on doing this recursively.
      - GPUs don't really support recursion. So how to map this quicksort.
      - You can implement this in the GPU as follows:
        - COMPACT it by comparing the pivot element to each number and move the
          numbers first less than (<), then equal(=) and finally greater than
          pivot. Then repeat these operations on individual SEGMENTS.
      - We will talk about calling Kernels within kernels in the next few lessons.

      Key value sort:
        - Maintain the keys but keep pointers to values to make it easy to move
          them around.

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

Lesson 5:
  - To recap from lesson 2 GPU programmers have the following goals to optimize:
    - Increase arithmetic intensity by reducing time spent on memory operations.
    - Coalesce global memory access.
    - Avoid thread divergence.

  - Levels of optimization:
    1. Picking good algorithms (Merge sort vs Insertion sort)
    2. Write cache aware code (Traverse row wise in a row ordered 2D array)
    3. Arch specific detailed optimizations (Vector registers SSE,AVX)
      - Not really needed a lot for GPU. The speedups gained by these are usually
        minor.
      - This really does matter for the CPU though.
    4. Micro optimizations at the instruction level. (Fast inverse square root : Quake 3)

  - In this lesson we're gonna look at the following at the levels mentioned above:
    1. Fundamentally parallel.
    2. Use shared memory and coalescing global memory.
    3. "Bank conflicts" and optimizing registers.
    4. Floating point denorm hacks.

  - You should have a systematic optimization process.

  - APOD:
    - Analyze
    - Parallelize
    - optimize
    - Deploy

  - Analyze: Profile whole application and find where it can benefit from parallelization
    and by how much can it benefit.
  - Parallelize: Pick an approach (OpenMP, CUDA, OpenACC) and pick the correct
    algorithm
  - Optimize: Profile driven optimization
  - Deploy: Don't optimise in a vaccum! Push it to real deployment to see if it
    helps.

  - Weak vs Strong scaling:
    - Weak: Run a larger problem in the same time
    - Strong: Run the problem faster

  - Understanding hotspots:
    - Run a profiler: gprop, vTune, verySleepy
    - Super important to understand where to parallelize the code.
    - Amdahl's Law:
      - Total speedup possible from parallelizing a part is given by 1/(1-p) where
        p is the percentage of time spent in that portion.
      - Eg: 50% -> 1/(1-0.5) = 2;

  - Check out the transpose.cu code in the Lesson5 folder to see how
    parallelization helps.
