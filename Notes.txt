XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

Lesson1:

- How to speed up your computation:
  - Faster clock : Shorter time for computation but way more power.
  - More work per cycle: There is a limit to the instruction level parallelism
  - Parallelism: Many smaller simpler processors doing the tasks.

- Modern GPUs
 - Thousands of ALUs
 - Hundereds of processors
 - Tens of thousands of concurrent threads

- GPGPU = General processing on a graphics processing units

- Transistors and becoming smaller but the clock frequency is becoming flat. So
  are getting faster performance because more transistors are there not because
  of clock speed.
- But more transistors means more power and more heat. That's a problem.

- CPU : Have a complex control hardware which means higher performance and flexibility
        but expensive in terms of power.
- GPU : Simpler control hardware means power efficient, more HW for computation
        but less flexible and more restrictive.

- Note that Latency and Throughput are the key factors. CPU optimizes for Latency.
  It wants to complete the tasks quicker. But GPU wants to process as many tasks as
  possible at a time and hence optimizes for throughput.

- GPU design tenets:
  - Lots of simple compute units. Trade simple control for more compute
  - Explicitly cater to parallel programming.
  - Optimize for throughput not latency.

- CUDA:
  - Allows us to program both the CPU and GPU in one program.
  - CPU is the host. It will be in plain C.
  - GPU is the device. It will also be in C but with some extensions.
  - The compiler will split the code and run the respective parts on CPU/GPU.
  - Assumes that the CPU/GPU have separate memories.
  - CPU runs the main program and performs the following:
    - Control data flows from CPU to GPU (cuda memcpy)
    - Control data flows from GPU to CPU (cuda memcpy)
    - Allocate memory on the GPU (cuda malloc)
    - Launch kernel on the GPU
  - NOTE that the CPU is the boss.

- Typical GPU program
  1. CPU allocates storage on GPU.
  2. CPU copies data from the CPU to the GPU.
  3. CPU launches kernels (equal to number of threads) on GPU to process the data.
  4. CPU copies results from the CPU back to the GPU.

- GPU programming is favored for applications with a good ratio of processing to
  communication.

- BIG IDEA OF CUDA:
  - Kernels look like serial programs. You write programs as if it will run on
    one thread but the GPU will run that parallel block on many threads. (SPMD)
  - GPUs are good at:
    - Efficiently launching lots of threads.
    - Running lots of threads in parallel.

- Running a kernel
  - The first is the number of blocks and the second is the number of threads per block
  - Can run many blocks at once.
  - Max number of threads per block is 512 for old and 1024 for new
  kernel<<<BLOCK_SIZE, THREADS_PER_BLOCK>>>(VARIABLES);
  - Each thread knows its index in the block and also knows the index of the block
  - Both of the parameters above can be 1,2 or 3 dimensional. For eg. If you
    have 128x128 image you could have 1x128 blocks of 128x1 threads or you could
    have a 8x8 grid of blocks where each block is of size 8x8
  kernel<<<GRID_OF_BLOCKS, BLOCKS_OF_THREADS>>>(VARIABLES);
  - You can use the dim3(x,y,z) to define the dimensionality of each of those
    parameters. It defaults y and z to 1 when not specified.
  kernel<<<dim3(bx,by,bz), dim3(tx,ty,tz), shmem>>>(VARIABLES)
  - NOTE: Why are you able to pass some host variables directly (like numRows and
    numCols in all the image kernels)
      https://stackoverflow.com/questions/6499036/kernel-parameter-passing-in-cuda
  - Here we have
    - Dimensionality of thread of blocks
    - Dimensionality of block of threads
    - Shared memory per block in bytes (defaulted to 0)
  - Each thread knows
    - threadIdx: x,y,z dimensions of the thread in the block.
    - blockDim: Size of the block of threads (How many threads in block)
    - blockIdx: x,y,z dimensions of block in the grid
    - gridDim: Size of the grid of blocks
  - A __device__ or __global__ function runs on the GPU /Device functions can only
    be called from other device or global functions. __device__ functions cannot
    be called from host code.
  - Note that in each SM the threads are aggregated into sets of 32 called warps.
    The warps run the same code and run in an SIMD fashion. This fact can sometimes
    be used to optimize your functions.
      - https://stackoverflow.com/questions/10460742/how-do-cuda-blocks-warps-threads-map-onto-cuda-cores
      - https://stackoverflow.com/questions/10729185/removing-syncthreads-in-cuda-warp-level-reduction

- Map
  - Here we have elements we want to process in parallel
  - There is a function we use to process each element
  - So we have an abstraction Map(ELEMENTS, FUNCTION)
  - GPUs are good at maps:
    - GPUS have many parallel processors
    - Its optimized for throughput and not latency

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

Lesson 2:

- Threads are working together on the same shared memory:
  - They may be accessing mutually exclusive data.
  - Dumping all of their results to the same spot.
  - Sharing partial results between them.

- Different patterns of communication:
  - Map:
    - What we've done till now. Tasks read and write to specific data elements
    - Like pixels in a image or indexes in an array.
    - One to one correspondence input and out.
    - Not flexible. Like can't average.
  - Gather:
    - Many input to one output.
    - Averaging over the window in an array.
    - Mean filtering over an image.
  - Scatter:
    - One input is spread across different output.
    - Like spreading the pixel value among surrounding pixels.
    - Scattering the results over the memory.
    - Problem several threads are trying to access the same memory at same time.
    - Each thread has to compute where to write the result.
  NOTE: The main difference between gather and scatter is that in gather non adjacent
  memory locations are being mapped to adjacent memory locations. In scatter on the
  other hand adjacent input locations are being mapped to memory locations that are
  computed using some criteria.
  - Stencil:
    - Tasks read input from a fixed neighborhood in an array
    - There is often a lot of data reuse when the stencil masks overlap
    - Each cell is of course gonna be read as many times as there are elements in
      a stencil.
    - A stencil must generate an output for every input.
  - Transpose:
    - Transposing an array expressed in row major order to a column major order.
    - It can be expressed as a gather or scatter (see NOTE above for how its different.)
    - Can be operated on arrays, matrices, images, data structures (Combining certain
      fields together to get a Structure of Arrays from an Array of Structures)
    - Tasks reorder data elements in memory.
    - It is one to one.
  NOTE:
    - Map and transpose are one to one.
    - A stencil needs to have as many outputs as input. Its a special form of gather.
    - Gather (Many to one). Mapped to adjacent memory location.
    - Scatter (one to many). Need to compute where data is being mapped to.
    - Reduce (all to one) like sum or average
    - Scan / Sort (all to all) All of the input can affect the destination of the output

- GPU Hardware:
  - Divide into smaller computations called kernel.
  - Thread blocks are a group of threads that cooperate to solve a subproblem.

- Threads, blocks and SMs:
  - A GPU is a collection of streaming multiprocessors or SM.
  - A SM has a bunch of simple processors and a memory.
  - When you have threads arranged in blocks the GPU will take care of assigning
    the blocks to the SM.
  NOTE: An SM can run more than one thread block but a thread block cannot run in
  more than one SM. Also note that only threads in the same block may cooperate.
  - So CUDA makes few guarantees about when and where the thread blocks can be run.
  - This flexibility allows hardware to run efficiently, no waiting and scalability.
  - The cons are there are no communication between blocks and no assumption on where
    blocks are running.
  NOTE: A dead lock is when one thread is waiting for the result of another but the
  thread has already been completed.
  - Threads and blocks must complete. They cannot be hanging around forever.
  NOTE: cudaDeviceSynchronize() ensure all the blocks execute.

- CUDA guarantees that
  - All threads ina block run on the same SM at the same time.
  - All the blocks of 1 kernel finish before the blocks of the next kernel are run.

- Memory model:
  - All the threads have local memory.
  - The threads in thread blocks have access to same shared memory which sits on the SM directly.
  - There is also global memory which is accessible to threads everywhere.
  - This is the GPU. The CPU of course has its own host memory.
  - Usually data is copied from the host to the global memory before launching kernels
    to work on them.

- Synchronization:
  - Having this shared and global memory means that threads can work together.
  - But what if one attempt to read a result before another writes it. Hence the
    need to synchronize.
  - Barrier:
    - Point in the program where threads wait till all the threads have reached the
      point.
    - NOTE: Each read write from shared memory needs to be followed up by a barrier.
      Eg:
        // MOving values one idx left in a shared array
        int idx = threadIdx.x;
        __shared__ int array[128]; // Again note this is shared among threads in the same block

        array[idx] = idx; // Writing : Need to sync threads
        __syncThreads();

        if(idx<127)
        {
          // Important to separate reads and writes from shared memories to prevent collisions
          int temp  = array[idx+1]; // Reading : Need to sync threads
          __syncThreads();

          array[idx] = temp; // Writing : Need to sync threads
          __syncThreads();
        }
      - There is an implicit barrier between kernels. All blocks in a kernel must
        end before the next thread executes.

- High level stratergies:

  1. Try to be smart with memory
  - Maximize the math per memory by either upping the math per thread or
    reducing the time spent on memory access.
  - Local>Shared>>Global>>CPU(host) in terms of speed.
  - So you wanna move frequently accessed data to fast memory.
  - Another way is to use a coalesce the access to global memory.
    - When a thread wants to use some global memory it will usually grab a chunk
      of data around it. Thus if surrounding threads are accessing contiguous data
      this chunk of global memory can be reused. Data is coalesced.
    - If the threads are accessing scattered data (strided or random) then more memory
      transactions ie more chunks need to be accessed.

    2. Avoid thread divergence
    - Threads that do different things due to loops dependent on the id or some
      conditional logic or any reason for them to not be performing the exact same
      operation
    - The GPU likes to perform the same operations on all threads. So some efficiency
      is lost when some threads have more operations than others.


- Atomic:
  - Helps avoid race conditions (collisions) where different threads are trying to read write
    modify the same memory.
  - Some examples are atomicAdd(), atomicMin(), atomicCAS() etc.
  - Limitations:
    - Only certain operations and data types are supported
    - You can work around this using the atomicCAS(). Check it out when possible.
    - No ordering constraint. Problem is that floating point arithmetic is not associative.
    - Serializes the operations which can make atomic very slow.

- NOTE: So when you're evaluating time look for both memory constraints and
  synchronization schemes used.

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

Lesson 3:

Complexity:
  - Ideal scaling: Work reduces linearly with linear increase in processors.
  - Need to talk about step complexity and work complexity.
  - Step complexity is the number of steps in an algorithm.
  - Work complexity is the sum of individual units done by all.
  - A parallel algorithm is work efficient if its work complexity is asymptotically the same.

Reduce:
  - All to one like a sum
  - It has 2 inputs:
    - Set of elements
    - Reduction operator that operates on the arrays
  - Operators need to have the characteristics:
    - Binary : Operations should be on 2 vals only.
    - Associative : (a+b)+c = a+(b+c) where + is any operator
    - Example : Multiply, add, logical or, logical and, min, max
  - Serial operations for reductions take n-1 operations on an array size.
    The work and step complexity is same n-1 or O(n)
  - Parallel reduce:
    - Sequential  reduce ((a+b)+c)+d = (a+b) + (c+d) Parallel reduce
    - This is why associative property was a must.
    - Now both have same work of 3 operations (+) but if you formulate the steps
      because (a+b) and (c+d) can be done in parallel the steps reduces to 2 from
      3.
    - The step complexity has gone down from n to logn. But in real life you
      don't always have n/2 processors to do n/2 operations in the first step.
  - NOTE: See reduce.cu on how to do the reduce method. Also note the difference
    in using shared memory vs global memory. Notice how you're incurring that
    additional copy penalty of copying the global data to shared memory of a thread
    block in order to have faster access and not have to do global memory lookups.
    Syntax for shared memory kernel:
    shmem_reduce_kernel<<<blocks, threads, threads * sizeof(float)>>>
        (d_out, d_in);
    Here the 3rd entry in the bracket is the shared memory size you wanna allocate
    which is of course equal to the number of threads times the size of the datatype
    you're working with.
  - The shared memory variant in reduce.cu is doing 3 times less read write combos
    to the global memory. In the global memory case in each loop you are reading from
    and writing to global memory and you get a GP of reads and writes as you progress
    in steps. But in the shared memory case you are only reading all the global values once
    (1024 reads) and you are writing to global memory only once (1 write) per kernel.
  - Suggested optimizations beyond just converting to shared memory:
    - Process multiple items per thread instead of just one. (meaning?? Maybe run fewer blocks and compute more than one entry in intermediate)
    - Perform the first step of the reduction when copying data from global to shared (trivial)
    - Utilize the fact that warp threads finish synchronously.
      - https://stackoverflow.com/questions/10729185/removing-syncthreads-in-cuda-warp-level-reduction

  Scan:
    - It basically applies an operation like a scan. So the ouput at an index depends
      on all the elements BEFORE (MAY OR MAY NOT including it itself).
    - Example for ADD operation:
      - Input : 1 2 3 4
      - Output: 1 3 6 10
    - This might seem difficult to parallelize.
    - Lots of uses of scan such as compaction and allocation (What are they?)
    - Scan has also been used for sparse matrix optimization, quicksort and data compression.
    - Inputs:
      - Array (Like reduce)
      - Binary associative operator (Like reduce)
      - Also needs an identity element.
    - There are 2 types of scan:
      - Exclusive: Is dependent on all the elements before it NOT including it.
                  (Hence first element is identity)
      - Inclusive: Is dependent on all the elements before it AND including it.
    1. So one stratergy for an inclusive scan might be:
      - For the k-th element run reduce for 0->k. Hence for a n element array
        run reduce n times.
      - Thus work is going to be O(n^2) (n*(n+1)/2 operations)
      - But steps will still be bounded by the max reduce operation which will
        be for element n. So O(logn).
      - Super inefficient in terms of work.
    2. Hillis/Steele inclusive scan
      - Procedure:
        - From 2 to n add 1 element to your left. Keep the elements with no left
          neighbour in this case just the first element as is.
        - Next for 3 to n add the element 2 to the left and keep 1 and 2 as is
        - Next for 5 to n add the element 4 to the left and keep 1->4 as is
        - Repeat this doubling to 8 16 ... n/2 and basically add i to 2^i left neighbor
      - Eg: For n=8 it'll be just 3 steps and you'll get the scan.
      - The total work is max n elements times the number of steps which is logn
        So work is nlogn.
      - The step size is logn since the neighbour access step is jumping by a factor
        of 2.
      - Video: https://www.youtube.com/watch?v=RdfmxfZBHpo
    3. Blelloch exclusive scan
      - 2 steps: Reduce and Downsweep
      - First step is to do the reduce as normal. While keeping track of the positions
      - Then for downsweep look at this video. Its kinda complex to explain verbally.
        Its just like a reverse reduction using the values gathered in the reduce steps.
      - Video: https://www.youtube.com/watch?v=mmYv3Haj6uc
      - The reduce phase has O(logn) step (Has 2 logn steps) complexity and O(n)
        work complexity.
      - So this scan method has double the number of steps than Hillis/Steele but
        considerably less work than it.

    - How to choose between 2. and 3. ?
      - Hillis Steele is better in the number of steps but Blelloch has better work complexity.
      - If more work than processors. Then we prefer 3. Else prefer 2.
      - A lot of algorithms have different work/processor ratio. So switch between
        these depending on this ratio at that point.

    - Summary:
      - Serial : n steps n work
      - Hillis steele: logn steps nlogn work
      - Blelloch: 2logn steps n work

  Histogram:
    - Cumulative distribution: Sum of bin values before me. Use exclusive scan.
    - Naively updating bins in threads will lead to race conditions which are collisions.
    - A simple increment involves reading the bin and storing it in a register, incrementing
      in a local register and writing the bin value back to global memory. So if multiple
      threads clash at these ops you're bound to get wrong bin values.
    - 3 ways to solve it here:
      - 1. Atomics:
        What if we could combine the 3 ops and make it 1 op. This would prevent
        clashing. This is possible through atomic. It basically locks down the
        global memory location while another thread is doing the 3 ops. Diadvantage
        it serializes the bin access since only one thread can access a bin memory location
        at a time. The higher the bins the less chances of collisions. Look at histo.cu for this.
      - 2.  Per thread histograms, then reduce:
        If we have n elements b bins and t threads. Make a local histogram per
        thread where each histogram is made by SERIALLY allocating n/t elements into
        b bins. Then just combine the bins individually using reduce on the histograms.
        It would be faster than running the reduce on the bins.
      - 3. Sort, then reduce by key:
        - Think of entries in key value pairs where the key is the bin and value
          is the value. So you sort by key(bins) first and then reduce all the entries
          of the same bin to get the final value of that bin.
        - Will be discussed in future lessons.
    -
