Lesson1:
- How to speed up your computation:
  - Faster clock : Shorter time for computation but way more power.
  - More work per cycle: There is a limit to the instruction level parallelism
  - Parallelism: Many smaller simpler processors doing the tasks.

- Modern GPUs
 - Thousands of ALUs
 - Hundereds of processors
 - Tens of thousands of concurrent threads

- GPGPU = General processing on a graphics processing units

- Transistors and becoming smaller but the clock frequency is becoming flat. So
  are getting faster performance because more transistors are there not because
  of clock speed.
- But more transistors means more power and more heat. That's a problem.

- CPU : Have a complex control hardware which means higher performance and flexibility
        but expensive in terms of power.
- GPU : Simpler control hardware means power efficient, more HW for computation
        but less flexible and more restrictive.

- Note that Latency and Throughput are the key factors. CPU optimizes for Latency.
  It wants to complete the tasks quicker. But GPU wants to process as many tasks as
  possible at a time and hence optimizes for throughput.

- GPU design tenets:
  - Lots of simple compute units. Trade simple control for more compute
  - Explicitly cater to parallel programming.
  - Optimize for throughput not latency.

- CUDA:
  - Allows us to program both the CPU and GPU in one program.
  - CPU is the host. It will be in plain C.
  - GPU is the device. It will also be in C but with some extensions.
  - The compiler will split the code and run the respective parts on CPU/GPU.
  - Assumes that the CPU/GPU have separate memories.
  - CPU runs the main program and performs the following:
    - Control data flows from CPU to GPU (cuda memcpy)
    - Control data flows from GPU to CPU (cuda memcpy)
    - Allocate memory on the GPU (cuda malloc)
    - Launch kernel on the GPU
  - NOTE that the CPU is the boss.

- Typical GPU program
  1. CPU allocates storage on GPU.
  2. CPU copies data from the CPU to the GPU.
  3. CPU launches kernels (equal to number of threads) on GPU to process the data.
  4. CPU copies results from the CPU back to the GPU.

- GPU programming is favored for applications with a good ratio of processing to
  communication.

- BIG IDEA OF CUDA:
  - Kernels look like serial programs. You write programs as if it will run on
    one thread but the GPU will run that parallel block on many threads. (SPMD)
  - GPUs are good at:
    - Efficiently launching lots of threads.
    - Running lots of threads in parallel.

- Running a kernel
  - The first is the number of blocks and the second is the number of threads per block
  - Can run many blocks at once.
  - Max number of threads per block is 512 for old and 1024 for new
  kernel<<<BLOCK_SIZE, THREADS_PER_BLOCK>>>(VARIABLES);
  - Each thread knows its index in the block and also knows the index of the block
  - Both of the parameters above can be 1,2 or 3 dimensional. For eg. If you
    have 128x128 image you could have 1x128 blocks of 128x1 threads or you could
    have a 8x8 grid of blocks where each block is of size 8x8
  kernel<<<GRID_OF_BLOCKS, BLOCKS_OF_THREADS>>>(VARIABLES);
  - You can use the dim3(x,y,z) to define the dimensionality of each of those
    parameters. It defaults y and z to 1 when not specified.
  kernel<<<dim3(bx,by,bz), dim3(tx,ty,tz), shmem>>>(VARIABLES)
  - Here we have
    - Dimensionality of thread of blocks
    - Dimensionality of block of threads
    - Shared memory per block in bytes (defaulted to 0)
  - Each thread knows
    - threadIdx: x,y,z dimensions of the thread in the block.
    - blockDim: Size of the block of threads (How many threads in block)
    - blockIdx: x,y,z dimensions of block in the grid
    - gridDim: Size of the grid of blocks

- Map
  - Here we have elements we want to process in parallel
  - There is a function we use to process each element
  - So we have an abstraction Map(ELEMENTS, FUNCTION)
  - GPUs are good at maps:
    - GPUS have many parallel processors
    - Its optimized for throughput and not latency
